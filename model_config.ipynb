
custom1 = tf.keras.callbacks.LearningRateScheduler(custom)
print_lr = printLearningRate()
lr_dict = {}
lrate_stepped = tf.keras.callbacks.LearningRateScheduler(step_decay)

tf.keras.backend.clear_session()
load_model = False
model_load_from = 'ActualModels/NSRDB-PSM-Models/CNN_LSTM'

#----------------------------------------------- Config ---------------------------------------------------------
model_save_to = 'ActualModels/NSRDB-PSM-Models/DeepLSTM/CNN_LSTM-{epoch:03d}'
csv_save_to = '.csv'
best_model_save_to = 'ActualModels/NSRDB-PSM-Models/CNN_LSTM-{epoch:03d}'

csvLogger = tf.keras.callbacks.CSVLogger(csv_save_to, separator=",", append=True)
model_checkpoint_callback= tf.keras.callbacks.ModelCheckpoint(
                                      filepath= model_save_to,monitor= "val_mae", verbose=1,
                                      save_best_only=False,save_weights_only=False,mode="auto",save_freq="epoch")

best_model_checkpoint_callback= tf.keras.callbacks.ModelCheckpoint(
                                      filepath= best_model_save_to ,monitor= "val_mae", verbose=1,
                                      save_best_only=True,save_weights_only=False,mode="auto",save_freq="epoch")

# ONLY FOR USE IF USING ADAM-W
step = tf.Variable(0, trainable=False)
LRschedule = tf.optimizers.schedules.PiecewiseConstantDecay([1, 40], [1e-3, 1e-3, 3e-4])
WDschedule = tf.optimizers.schedules.PiecewiseConstantDecay([10,20], [0, 0, 0]) #wd
# lr and wd can be a function or a tensor
lr = LRschedule(step)
wd = WDschedule(step)


compileConfig = {'optimizer':tf.keras.optimizers.Nadam(1e-3), 'loss':tf.keras.losses.Huber(),
                 'metrics':['mse','mae','mape']}

fitConfig = {'x':rhum_train_window,'epochs':250,
             'validation_data':rhum_valid_window,
             'callbacks':[print_lr,lrate_stepped,best_model_checkpoint_callback]}

model_choice = 4
